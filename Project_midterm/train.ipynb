{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all necessary parts to train and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hydra\n",
    "import hydra.conf\n",
    "import wandb\n",
    "import numpy as np\n",
    "import os\n",
    "from omegaconf import DictConfig\n",
    "import utils\n",
    "from model import LinearRegression, Perceptron, LogisticRegression, MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data and hyperparameters configs, split training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@hydra.main(version_base=\"1.3\", config_path=\"./conf\", config_name=\"config_proj_midterm\")\n",
    "def main(cfg: DictConfig):\n",
    "    # Preprocess dataset\n",
    "    dataset_path = cfg.dataset\n",
    "    print(\"If path is existed:\", os.path.exists(dataset_path))\n",
    "    X, y = utils.load_and_process_data(dataset_path, features_to_remove=None) \n",
    "    X_train, X_test, y_train, y_test = utils.split_data(X[:,1:], y, test_size=0.3, val_size=0.2, random_state=42)\n",
    "\n",
    "    if(cfg.wandb_on_off and cfg.name == \"Project_midterm\"):\n",
    "        wandb.init(project=\"Project_midterm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression model implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression\n",
    "model_L = LinearRegression(n_feature=X_train.shape[1], epoch = cfg.epoch, lr = cfg.lr, gd = cfg.gd)\n",
    "model_L.fit(X_train, y_train)\n",
    "metrics_L = model_L._evaluate(X_test, y_test)\n",
    "print(f\"Linear Regression evaluation: {metrics_L}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perceptron model implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perceptron\n",
    "y_train_P = y_train.copy()\n",
    "y_test_P = y_test.copy()\n",
    "y_train_P[y_train_P == 0] = -1\n",
    "y_test_P[y_test_P == 0] = -1\n",
    "model_P = Perceptron(n_feature=X_train.shape[1], epoch=1000, lr=cfg.lr, tol=cfg.tol, wandb=cfg.wandb_on_off, gd=cfg.gd)\n",
    "model_P.fit(X_train, y_train_P)\n",
    "metrics_P = model_P._evaluate(X_test, y_test_P)\n",
    "print(f\"Perceptron evaluation: {metrics_P}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression model implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "model_LR = LogisticRegression(n_feature=X_train.shape[1], epoch=cfg.epoch, lr=cfg.lr, tol=cfg.tol, wandb=cfg.wandb_on_off, gd=cfg.gd)\n",
    "model_LR.fit(X_train, y_train)\n",
    "metrics_LR = model_LR._evaluate(X_test, y_test)\n",
    "print(f\"Logistic Regression evaluation: {metrics_LR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLP model implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP\n",
    "input_size = X.shape[1] - 1 \n",
    "layers_list = [input_size, 10, 1]\n",
    "model_MLP = MLP(layers_list)\n",
    "metrics_MLP = utils.cross_validate(model_MLP, X, y, cfg.k, cfg.epoch, cfg.lr, cfg.batch_size, cfg.gd)\n",
    "print(f\"MLP evaluation: {metrics_MLP}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results with all the features using 4 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression evaluation:{'accuracy': 0.031, 'recall': 0.031, 'precision': 0.000961, 'f1': 0.0018642095053346267}\n",
      "Perceptron evaluation: {'accuracy': 0.031, 'recall': 0.031, 'precision': 0.000961, 'f1': 0.0018642095053346267}\n",
      "Logistic Regression evaluation: {'accuracy': 0.969, 'recall': 0.969, 'precision': 0.9389609999999999, 'f1': 0.9537440325038092} \n",
      "MLP evaluation: {'accuracy': 0.9661, 'recall': 0.9661, 'precision': 0.9338183500000001, 'f1': 0.9495677504357161}\n"
     ]
    }
   ],
   "source": [
    "print(\"Linear Regression evaluation:{'accuracy': 0.031, 'recall': 0.031, 'precision': 0.000961, 'f1': 0.0018642095053346267}\\nPerceptron evaluation: {'accuracy': 0.031, 'recall': 0.031, 'precision': 0.000961, 'f1': 0.0018642095053346267}\\nLogistic Regression evaluation: {'accuracy': 0.969, 'recall': 0.969, 'precision': 0.9389609999999999, 'f1': 0.9537440325038092} \\nMLP evaluation: {'accuracy': 0.9661, 'recall': 0.9661, 'precision': 0.9338183500000001, 'f1': 0.9495677504357161}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using different features to train the logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainning and testing sets without two temperature features\n",
    "X_train_LR1 = X_train[:,2:]\n",
    "X_test_LR1 = X_test[:,2:]\n",
    "# trainning and testing sets with only two temperature features\n",
    "X_train_LR2 = X_train[:,0:2]\n",
    "X_test_LR2 = X_test[:,0:2]\n",
    "model_LR = LogisticRegression(n_feature=X_train.shape[1], epoch=cfg.epoch, lr=cfg.lr, tol=cfg.tol, wandb=cfg.wandb_on_off, gd=cfg.gd)\n",
    "model_LR1 = LogisticRegression(n_feature=X_train_LR1.shape[1], epoch=cfg.epoch, lr=cfg.lr, tol=cfg.tol, wandb=cfg.wandb_on_off, gd=cfg.gd)\n",
    "model_LR2 = LogisticRegression(n_feature=X_train_LR2.shape[1], epoch=cfg.epoch, lr=cfg.lr, tol=cfg.tol, wandb=cfg.wandb_on_off, gd=cfg.gd)\n",
    "model_LR.fit(X_train, y_train)\n",
    "model_LR1.fit(X_train_LR1, y_train)\n",
    "model_LR2.fit(X_train_LR2, y_train)\n",
    "# Evaluate each model for comparison\n",
    "metrics_LR = model_LR._evaluate(X_test, y_test)\n",
    "metrics_LR1 = model_LR1._evaluate(X_test_LR1, y_test)\n",
    "metrics_LR2 = model_LR2._evaluate(X_test_LR2, y_test)\n",
    "print(f\"Logistic Regression evaluation with all features: {metrics_LR}\")\n",
    "print(f\"Logistic Regression evaluation without two temperature features: {metrics_LR1}\")\n",
    "print(f\"Logistic Regression evaluation with only two temperature features: {metrics_LR2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results with all the features using logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression evaluation with all features: {'accuracy': 0.969, 'recall': 0.969, 'precision': 0.9389609999999999, 'f1': 0.9537440325038092}\n",
      "Logistic Regression evaluation without two temperature features: {'accuracy': 0.8946666666666667, 'recall': 0.8946666666666667, 'precision': 0.9553094565487982, 'f1': 0.9209330197885173}\n",
      "Logistic Regression evaluation with only two temperature features: {'accuracy': 0.969, 'recall': 0.969, 'precision': 0.9389609999999999, 'f1': 0.9537440325038092}\n"
     ]
    }
   ],
   "source": [
    "print(\"Logistic Regression evaluation with all features: {'accuracy': 0.969, 'recall': 0.969, 'precision': 0.9389609999999999, 'f1': 0.9537440325038092}\\nLogistic Regression evaluation without two temperature features: {'accuracy': 0.8946666666666667, 'recall': 0.8946666666666667, 'precision': 0.9553094565487982, 'f1': 0.9209330197885173}\\nLogistic Regression evaluation with only two temperature features: {'accuracy': 0.969, 'recall': 0.969, 'precision': 0.9389609999999999, 'f1': 0.9537440325038092}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
